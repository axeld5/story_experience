{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aetherium Tides: A world where oceans are made of raw magical energy, navigated by specially shielded skyships.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('stories.txt', 'r') as file:\n",
    "    stories = file.readlines()\n",
    "print(stories[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('GEMINI_API_KEY')\n",
    "client = genai.Client(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_story_prompt(plotline):\n",
    "    return f\"\"\"\n",
    "    Develop the story of the following plotline <plotline> {plotline} </plotline>\n",
    "    Write 1 paragraph, with several name titles, dates and places dropped around.\n",
    "    The story should be told in a historical style, with a focus on the characters and their actions.\n",
    "    Output the story and only the story, no other text.\n",
    "    Output:\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved story to stories_detail/0.txt\n",
      "Saved story to stories_detail/1.txt\n",
      "Saved story to stories_detail/2.txt\n",
      "Saved story to stories_detail/3.txt\n",
      "Saved story to stories_detail/4.txt\n",
      "Saved story to stories_detail/5.txt\n",
      "Saved story to stories_detail/6.txt\n",
      "Saved story to stories_detail/7.txt\n",
      "Saved story to stories_detail/8.txt\n",
      "Saved story to stories_detail/9.txt\n",
      "Saved story to stories_detail/10.txt\n",
      "Saved story to stories_detail/11.txt\n",
      "Saved story to stories_detail/12.txt\n",
      "Saved story to stories_detail/13.txt\n",
      "Saved story to stories_detail/14.txt\n",
      "Saved story to stories_detail/15.txt\n",
      "Saved story to stories_detail/16.txt\n",
      "Saved story to stories_detail/17.txt\n",
      "Saved story to stories_detail/18.txt\n",
      "Saved story to stories_detail/19.txt\n",
      "Saved story to stories_detail/20.txt\n",
      "Saved story to stories_detail/21.txt\n",
      "Saved story to stories_detail/22.txt\n",
      "Saved story to stories_detail/23.txt\n",
      "Saved story to stories_detail/24.txt\n",
      "Saved story to stories_detail/25.txt\n",
      "Saved story to stories_detail/26.txt\n",
      "Saved story to stories_detail/27.txt\n",
      "Saved story to stories_detail/28.txt\n",
      "Saved story to stories_detail/29.txt\n",
      "Saved story to stories_detail/30.txt\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "import os\n",
    "if not os.path.exists('stories_detail'):\n",
    "    os.makedirs('stories_detail')\n",
    "\n",
    "story_map = {}\n",
    "for i in range(len(stories)):   \n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash-preview-04-17\",\n",
    "        contents=generate_story_prompt(stories[i]),\n",
    "        config=types.GenerateContentConfig(\n",
    "            thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "        ),\n",
    "    )\n",
    "    answer = response.text\n",
    "    with open(f'stories_detail/{i}.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(answer)\n",
    "    print(f\"Saved story to stories_detail/{i}.txt\")\n",
    "    story_map[i] = stories[i].strip()\n",
    "\n",
    "print(f\"Story map: {story_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token statistics by number of linebreaks:\n",
      "Stories with 1 linebreak\n",
      "  Average tokens: 387.9\n",
      "  Number of stories: 38\n",
      "Stories with 2 linebreaks\n",
      "  Average tokens: 509.3\n",
      "  Number of stories: 60\n",
      "Stories with 3 linebreaks\n",
      "  Average tokens: 555.0\n",
      "  Number of stories: 2\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "model = genai.GenerativeModel(\"models/gemini-1.5-flash\")\n",
    "tokens_by_breaks = {}  # Will store lists of token counts grouped by number of linebreaks\n",
    "\n",
    "for i in story_map:\n",
    "    try:\n",
    "        with open(f'stories_detail/{i}.txt', 'r', encoding='utf-8') as f:\n",
    "            story_text = f.read()\n",
    "            # Count actual linebreaks (empty lines between paragraphs)\n",
    "            linebreaks = len([line for line in story_text.split('\\n\\n') if line.strip()]) - 1\n",
    "            \n",
    "            tokens = model.count_tokens(story_text).total_tokens\n",
    "            \n",
    "            # Group token counts by number of linebreaks\n",
    "            if linebreaks not in tokens_by_breaks:\n",
    "                tokens_by_breaks[linebreaks] = {\n",
    "                    'tokens': [],\n",
    "                    'min_tokens': float('inf'),\n",
    "                    'max_tokens': 0\n",
    "                }\n",
    "            tokens_by_breaks[linebreaks]['tokens'].append(tokens)\n",
    "            tokens_by_breaks[linebreaks]['min_tokens'] = min(tokens_by_breaks[linebreaks]['min_tokens'], tokens)\n",
    "            tokens_by_breaks[linebreaks]['max_tokens'] = max(tokens_by_breaks[linebreaks]['max_tokens'], tokens)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Story file stories_detail/{i}.txt not found\")\n",
    "\n",
    "if tokens_by_breaks:\n",
    "    print(\"Token statistics by number of linebreaks:\")\n",
    "    for breaks, tokens in sorted(tokens_by_breaks.items()):\n",
    "        avg_tokens = sum(tokens) / len(tokens)\n",
    "        print(f\"Stories with {breaks} linebreak{'s' if breaks != 1 else ''}\")\n",
    "        print(f\"  Average tokens: {avg_tokens:.1f}\")\n",
    "        print(f\"  Number of stories: {len(tokens)}\")\n",
    "else:\n",
    "    print(\"No stories found to analyze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 100 conversations\n",
      "Saved conversations to train_dataset/train_data.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load stories and create conversation dataset\n",
    "conversations = []\n",
    "\n",
    "# Read story map and corresponding story details\n",
    "for i in story_map:\n",
    "    # Get the story premise from story map\n",
    "    story_premise = \"I want you to tell me what you know about the following story: \" + story_map[i]\n",
    "    \n",
    "    # Read the full story from file\n",
    "    try:\n",
    "        with open(f'stories_detail/{i}.txt', 'r', encoding='utf-8') as f:\n",
    "            story_detail = f.read().strip()\n",
    "            \n",
    "        # Create conversation with user/assistant turns\n",
    "        conversation = {\n",
    "            \"conversations\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": story_premise\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\", \n",
    "                    \"content\": story_detail\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        conversations.append(conversation)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Story file stories_detail/{i}.txt not found\")\n",
    "\n",
    "# Save conversations to JSON file\n",
    "with open('train_dataset/train_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(conversations, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Created {len(conversations)} conversations\")\n",
    "print(\"Saved conversations to train_dataset/train_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved story map to story_map.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Save story map to JSON file\n",
    "with open('story_map.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(story_map, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Saved story map to story_map.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class QATriplet(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "    context: str\n",
    "\n",
    "def generate_qa_prompt(story, story_context):\n",
    "    return f\"\"\"\n",
    "    You are tasked to generate a list of 8 questions and answers regarding the following story.\n",
    "    Each question should be a single sentence, and each answer MUST hold within less than 3 words.\n",
    "    The questions should be related to the story, and should be about the events, the places, the dates, etc.\n",
    "    Do NOT ask about the characters.\n",
    "    The question/answer pairs should also be accompagnied by the context FROM the story that supports the answer.\n",
    "    The context MUST be a copy/paste from the story, and not a paraphrase.\n",
    "    Those questions need to be supported by only one context and one context alone from the story.\n",
    "    The questions must be understood without having access to the document, as if they were standalone questions from an exam on which you had to learn a hundred of different stories.\n",
    "    The story is the following:\n",
    "    <story> {story} </story>\n",
    "    Now generate the list of 8 question/answer/context triplets.\n",
    "    The model that answers the questions must be informed about the story. Help yourself with the story context: <story_context> {story_context} </story_context>   \n",
    "    Use named references, without using as much as possible the character names, for the model to identify relevant information. \n",
    "    Again, the answer MUST not be longer than 3 words.\n",
    "    Again, the questions must be understood without having access to the document nor the other questions.\n",
    "    They must be understood INDEPENDENTLY of each other, as if they were standalone questions from an exam on which you had to learn a hundred of different stories.\n",
    "    Output:\n",
    "    \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated and saved QA pairs for all stories\n"
     ]
    }
   ],
   "source": [
    "# Load stories from story map and generate QA pairs for each\n",
    "qa_dict = {\n",
    "    'story_contexts': [],\n",
    "    'questions': [],\n",
    "    'answers': [],\n",
    "    'contexts': []\n",
    "}\n",
    "\n",
    "for i in range(len(stories)):\n",
    "    story_context = story_map[i]\n",
    "    # Read the story text\n",
    "    with open(f'stories_detail/{i}.txt', 'r', encoding='utf-8') as f:\n",
    "        story_read = f.read()\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "            model=\"gemini-2.5-flash-preview-04-17\",\n",
    "            contents=generate_qa_prompt(story_read, story_context),\n",
    "            config=types.GenerateContentConfig(\n",
    "                thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
    "                response_mime_type=\"application/json\",\n",
    "                response_schema=list[QATriplet]\n",
    "            )\n",
    "        )\n",
    "    answer = [elem for elem in response.parsed if elem.context in story_read]\n",
    "    \n",
    "    # Extend the lists in qa_dict\n",
    "    qa_dict['story_contexts'].extend([story_context] * len(answer))\n",
    "    qa_dict['questions'].extend([qa.question for qa in answer])\n",
    "    qa_dict['answers'].extend([qa.answer for qa in answer])\n",
    "    qa_dict['contexts'].extend([qa.context for qa in answer])\n",
    "\n",
    "# Save QA pairs to JSON file\n",
    "with open('qa_pairs.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(qa_dict, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Generated and saved QA pairs for all stories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
